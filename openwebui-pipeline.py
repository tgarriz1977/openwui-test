# Pipeline Personalizado para Open WebUI
# Este pipeline intercepta las consultas y usa el servicio LlamaIndex con reranking

from typing import List, Optional, Dict, Any
from pydantic import BaseModel
import requests
import os

class Pipeline:
    """
    Pipeline avanzado de RAG con reranking para Open WebUI
    
    Flujo:
    1. Usuario hace pregunta en Open WebUI
    2. Pipeline intercepta la consulta
    3. Llama a LlamaIndex API con reranking habilitado
    4. Devuelve respuesta mejorada
    """
    
    class Valves(BaseModel):
        """Configuraci√≥n del pipeline"""
        LLAMAINDEX_API_URL: str = "http://llamaindex-api-service.rag-system.svc.cluster.local:8000"
        COLLECTION_NAME: str = "documents"
        ENABLE_RERANKER: bool = True
        TOP_K: int = 20
        RERANK_TOP_N: int = 5
        INCLUDE_SOURCES: bool = True
        MIN_SIMILARITY_SCORE: float = 0.7

    def __init__(self):
        self.valves = self.Valves()
        self.name = "RAG Pipeline con Reranking"
        self.description = "Pipeline avanzado de RAG usando LlamaIndex con BAAI reranker"

    async def on_startup(self):
        """Se ejecuta al iniciar el pipeline"""
        print(f"‚úÖ {self.name} iniciado")
        print(f"   API: {self.valves.LLAMAINDEX_API_URL}")
        print(f"   Reranker: {'Habilitado' if self.valves.ENABLE_RERANKER else 'Deshabilitado'}")

    async def on_shutdown(self):
        """Se ejecuta al detener el pipeline"""
        print(f"üõë {self.name} detenido")

    def pipe(
        self, 
        user_message: str, 
        model_id: str, 
        messages: List[dict], 
        body: dict
    ) -> str | Dict[str, Any]:
        """
        Procesa el mensaje del usuario
        
        Args:
            user_message: Mensaje actual del usuario
            model_id: ID del modelo seleccionado
            messages: Historial completo de conversaci√≥n
            body: Configuraci√≥n adicional
            
        Returns:
            Respuesta procesada con contexto RAG
        """
        
        try:
            # Verificar si hay documentos en la colecci√≥n
            collections_response = requests.get(
                f"{self.valves.LLAMAINDEX_API_URL}/collections",
                timeout=5
            )
            
            if collections_response.status_code != 200:
                return "‚ö†Ô∏è Error: No se puede conectar al servicio RAG"
            
            collections = collections_response.json().get("collections", [])
            
            if self.valves.COLLECTION_NAME not in collections:
                return f"‚ö†Ô∏è La colecci√≥n '{self.valves.COLLECTION_NAME}' no existe. Por favor, ingesta documentos primero."
            
            # Consultar con RAG
            query_payload = {
                "question": user_message,
                "collection": self.valves.COLLECTION_NAME,
                "use_reranker": self.valves.ENABLE_RERANKER,
                "top_k": self.valves.TOP_K,
                "rerank_top_n": self.valves.RERANK_TOP_N
            }
            
            response = requests.post(
                f"{self.valves.LLAMAINDEX_API_URL}/query",
                json=query_payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.json().get("detail", "Error desconocido")
                return f"‚ùå Error en RAG: {error_detail}"
            
            result = response.json()
            answer = result.get("answer", "")
            sources = result.get("sources", [])
            reranked = result.get("reranked", False)
            
            # Construir respuesta mejorada
            output = answer
            
            # Agregar informaci√≥n sobre el proceso
            if reranked:
                output += f"\n\nüîç *Respuesta mejorada con reranking*"
            
            # Agregar fuentes si est√° habilitado
            if self.valves.INCLUDE_SOURCES and sources:
                output += "\n\nüìö **Fuentes consultadas:**\n"
                for i, source in enumerate(sources, 1):
                    score = source.get("score", 0)
                    if score >= self.valves.MIN_SIMILARITY_SCORE:
                        text_preview = source.get("text", "")[:150]
                        metadata = source.get("metadata", {})
                        file_name = metadata.get("file_name", "Desconocido")
                        
                        output += f"\n{i}. **{file_name}** (relevancia: {score:.2f})\n"
                        output += f"   _{text_preview}..._\n"
            
            return output
            
        except requests.exceptions.Timeout:
            return "‚è±Ô∏è Timeout: La consulta tard√≥ demasiado. Intenta con una pregunta m√°s espec√≠fica."
        
        except requests.exceptions.ConnectionError:
            return "üîå Error de conexi√≥n: No se puede alcanzar el servicio RAG. Verifica que est√© ejecut√°ndose."
        
        except Exception as e:
            return f"‚ùå Error inesperado: {str(e)}"


# Funci√≥n auxiliar para configuraci√≥n avanzada
def get_advanced_config() -> dict:
    """
    Configuraci√≥n avanzada del pipeline
    Puedes modificar estos valores seg√∫n tus necesidades
    """
    return {
        "chunk_strategies": {
            "default": {"size": 1000, "overlap": 200},
            "precise": {"size": 500, "overlap": 100},
            "broad": {"size": 2000, "overlap": 400}
        },
        "retrieval_strategies": {
            "fast": {"top_k": 10, "rerank_top_n": 3},
            "balanced": {"top_k": 20, "rerank_top_n": 5},
            "comprehensive": {"top_k": 30, "rerank_top_n": 10}
        },
        "supported_collections": [
            "documents",
            "knowledge-base",
            "manuals",
            "policies"
        ]
    }
